m<-c()
for(i in colnames(df)){
x<-sum(is.na(df[,i]))
# missing value count
m<-append(m,x)
# non-missing value count
m<-append(m,nrow(df)-x)
}
# adding column and row names to matrix
a<-matrix(m,nrow=2)
rownames(a)<-c("TRUE","FALSE")
colnames(a)<-colnames(df)
return(a)
}
# Confusin matrix for supervised models
draw_confusion_matrix <- function(cm) {
layout(matrix(c(1,1,2)))
par(mar=c(2,2,2,2))
plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
title('CONFUSION MATRIX', cex.main=2)
# create the matrix
rect(150, 430, 240, 370, col='#3F97D0')
text(195, 435, 'CHURN', cex=1.2)
rect(250, 430, 340, 370, col='#F7AD50')
text(295, 435, 'NOCHURN', cex=1.2)
text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
text(245, 450, 'Actual', cex=1.3, font=2)
rect(150, 305, 240, 365, col='#F7AD50')
rect(250, 305, 340, 365, col='#3F97D0')
text(140, 400, 'CHURN', cex=1.2, srt=90)
text(140, 335, 'NOCHURN', cex=1.2, srt=90)
# add in the cm results
res <- as.numeric(cm$table)
text(195, 400, res[1], cex=1.6, font=2, col='white')
text(195, 335, res[2], cex=1.6, font=2, col='white')
text(295, 400, res[3], cex=1.6, font=2, col='white')
text(295, 335, res[4], cex=1.6, font=2, col='white')
# add in the specifics
plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
# add in the accuracy information
text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}
######### DATASETS #########
anagrafica <- read_csv("anagrafica.csv", show_col_types = FALSE,
col_types = cols(Provincia = col_skip(),
SiglaProvincia = col_skip(), Comune = col_skip()))
print(colSums(is.na(anagrafica)))
binMat = toBinaryMatrix(anagrafica)
barplot(binMat,
main = "Missing values in all features",xlab = "Frequency",
col = c("black","white"))
gg_miss_var(anagrafica, show_pct = TRUE)
gg_miss_upset(anagrafica)
######### DATASETS #########
accessi_app <- read_csv("accessi_app.csv", show_col_types = FALSE)
survey_start = as.Date(min(accessi_app$updated_at))
survey_end = as.Date(max(accessi_app$updated_at))
survey_period = survey_end - survey_start
survey_period
table(accessi_app$source)
accessi_app = accessi_app[- 2]
sum(is.na(accessi_app))
prodotti_caricati <- read_csv('prodotti_caricati.csv', show_col_types = FALSE)
prodotti_caricati = filter(prodotti_caricati, created_at >= survey_start & created_at <= survey_end)
print(colSums(is.na(prodotti_caricati)))
binMat = toBinaryMatrix(prodotti_caricati)
barplot(binMat,
main = "Missing values in all features",xlab = "Frequency",
col = c("black","white"))
gg_miss_var(prodotti_caricati, show_pct = TRUE)
missioni_players <- read_csv("missioni_players.csv", show_col_types = FALSE,
col_types = cols(type = col_skip()))
missioni_players = filter(missioni_players, created_at >= survey_start & created_at <= survey_end)
premi_mamme <- read_csv("premi_mamme.csv", show_col_types = FALSE,
col_types = cols(deliveryMode = col_skip(),
nomepremio = col_skip()))
premi_mamme = filter(premi_mamme, datarichiestapremio >= survey_start & datarichiestapremio <= survey_end)
conversione_ean_prodotto <- read_csv("conversione_ean_prodotto.csv",
col_types = cols(REFERENZA_DES = col_skip(),
SEGMENTO_DES = col_skip()))
last_access = accessi_app %>%
group_by(id_player)%>%
summarise(last_access = max(updated_at))
# Only players that have made at least one access
active_users = merge(last_access, anagrafica, all.x = TRUE)
active_users$last_time =  survey_end - as.Date(active_users$last_access)
active_users$last_time = as.double(active_users$last_time)
app_time = accessi_app %>%
group_by(id_player) %>%
summarise(tempo_app = as.Date(max(updated_at)) - as.Date(min(updated_at)))
# agiungo la variabile tempo app e la converto in double
active_users = merge(active_users, app_time, all.x = TRUE)
active_users$tempo_app = as.double(active_users$tempo_app)
prod_per_user = prodotti_caricati %>%
group_by(id_player)%>%
summarise(num_prod = n())
active_users = merge(active_users, prod_per_user, all.x = TRUE)
# Variabile categorica TIER
TIER_num = prodotti %>%
group_by(id_player, TIER) %>%
summarise(num = n())
prodotti = merge(prodotti_caricati, conversione_ean_prodotto, all.x = TRUE)
prodotti$TIER = as.factor(prodotti$TIER)
last_access = accessi_app %>%
group_by(id_player)%>%
summarise(last_access = max(updated_at))
# Only players that have made at least one access
active_users = merge(last_access, anagrafica, all.x = TRUE)
active_users$last_time =  survey_end - as.Date(active_users$last_access)
active_users$last_time = as.double(active_users$last_time)
- We can extract the interval of time between the first and the last access for every player:
app_time = accessi_app %>%
group_by(id_player) %>%
summarise(tempo_app = as.Date(max(updated_at)) - as.Date(min(updated_at)))
# agiungo la variabile tempo app e la converto in double
active_users = merge(active_users, app_time, all.x = TRUE)
active_users$tempo_app = as.double(active_users$tempo_app)
prod_per_user = prodotti_caricati %>%
group_by(id_player)%>%
summarise(num_prod = n())
active_users = merge(active_users, prod_per_user, all.x = TRUE)
# Variabile categorica TIER
TIER_num = prodotti %>%
group_by(id_player, TIER) %>%
summarise(num = n())
TIER1_num = filter(TIER_num, TIER == 'TIER1')
colnames(TIER1_num)[3] = 'TIER1'
TIER1_num = TIER1_num[-2]
TIER2_num = filter(TIER_num, TIER == 'TIER2')
colnames(TIER2_num)[3] = 'TIER2'
TIER2_num = TIER2_num[-2]
TIER3_num = filter(TIER_num, TIER == 'TIER3')
colnames(TIER3_num)[3] = 'TIER3'
TIER3_num = TIER3_num[-2]
active_users  = merge(active_users, TIER1_num, all.x = TRUE)
active_users  = merge(active_users, TIER2_num, all.x = TRUE)
active_users  = merge(active_users, TIER3_num, all.x = TRUE)
rewards = premi_mamme %>%
group_by(id_player)%>%
summarise(num_reward = n())
active_users = merge(active_users, rewards, all.x = TRUE)
tipo_premio = premi_mamme %>%
group_by(id_player, tipopremio) %>%
summarise(num = n())
basic = filter(tipo_premio, tipopremio == 'basic')
colnames(basic)[3] = 'basic'
basic = basic[-2]
special = filter(tipo_premio, tipopremio == 'special')
colnames(special)[3] = 'special'
special = special[-2]
active_users  = merge(active_users, basic, all.x = TRUE)
active_users  = merge(active_users, special, all.x = TRUE)
access_freq = accessi_app %>%
group_by(id_player)%>%
summarise(num_access = n())
active_users = merge(active_users, access_freq, all.x = TRUE)
missions_num = missioni_players %>%
group_by(id_player)%>%
summarise(num_missioni = n())
active_users = merge(active_users, missions_num, all.x = TRUE)
misstype_num = missioni_players %>%
group_by(id_player, subType) %>%
summarise(num = n())
cib_num = filter(misstype_num, subType == 'cib')
colnames(cib_num)[3] = 'cib'
cib_num = cib_num[-2]
double_num = filter(misstype_num, subType == 'double')
colnames(double_num)[3] = 'double'
double_num = double_num[-2]
ticket_num = filter(misstype_num, subType == 'ticket-punti')
colnames(ticket_num)[3] = 'ticket-punti'
ticket_num = ticket_num[-2]
active_users  = merge(active_users, cib_num, all.x = TRUE)
active_users  = merge(active_users, double_num, all.x = TRUE)
active_users  = merge(active_users, ticket_num, all.x = TRUE)
points_num = prodotti_caricati %>%
group_by(id_player)%>%
summarise(points_prodotti = sum(points))
active_users = merge(active_users, points_num, all.x = TRUE)
points_mission = missioni_players %>%
group_by(id_player)%>%
summarise(points_missioni = sum(points))
active_users = merge(active_users, points_mission, all.x = TRUE)
## Somma punti spesi per richiesta premi
points_expense = premi_mamme %>%
group_by(id_player)%>%
summarise(points_premi = sum(puntipremio))
active_users = merge(active_users, points_expense, all.x = TRUE)
## Tutti i NA per le colonne calcolate 'double' imposti = 0
active_users[8:length(active_users)][is.na(active_users[8:length(active_users)])] = 0
active_users = filter(active_users, tempo_app >= 7 & !is.na(ETA_MM_BambinoTODAY))
active_users = filter(active_users, ETA_MM_BambinoTODAY >-1 | num_prod != 0 | num_missioni != 0 | num_reward != 0)
active_users$week_access = round(active_users$num_access / (active_users$tempo_app/7), 2)
# Missioni
active_users$week_mission = round(active_users$num_missioni / (active_users$tempo_app/7), 2)
active_users$week_cib = round(active_users$cib / (active_users$tempo_app/7), 2)
active_users$week_double = round(active_users$double / (active_users$tempo_app/7), 2)
active_users$week_ticket = round(active_users$ticket / (active_users$tempo_app/7), 2)
# Prodotti caricati
active_users$week_prod = round(active_users$num_prod/(active_users$tempo_app/7), 2)
active_users$week_TIER1 = round(active_users$TIER1/(active_users$tempo_app/7), 2)
active_users$week_TIER2 = round(active_users$TIER2/(active_users$tempo_app/7), 2)
active_users$week_TIER3 = round(active_users$TIER3/(active_users$tempo_app/7), 2)
# Premi richiesti
active_users$week_reward = round(active_users$num_reward/(active_users$tempo_app/7), 2)
active_users$week_special = round(active_users$special/(active_users$tempo_app/7), 2)
active_users$week_basic = round(active_users$basic/(active_users$tempo_app/7), 2)
# Punti ottenuti e spesi
active_users$week_prodpoints = round((active_users$points_prodotti/100)/(active_users$tempo_app/7), 2)
active_users$week_misspoints = round((active_users$points_missioni/100)/(active_users$tempo_app/7), 2)
active_users$week_rewpoints = round((active_users$points_premi/100)/(active_users$tempo_app/7), 2)
library("png")
setwd("C:/Users/gltut/Desktop/Corsi/Statistical Data Analisys/FATER")
library("png")
pp <- readPNG("Churn.png")
data$days_toaccess = round(data$tempo_app / data$num_access, 2)
View(data)
active_users$days_toaccess = round(active_users$tempo_app / active_users$num_access, 2)
active_users$churn = 0
for (i in 1:dim(active_users)[1]){
ultimo_acc  = active_users$last_time[i]
media_accessi = active_users$days_toaccess[i] + sd(active_users$days_toaccess)
if ( ultimo_acc > media_accessi){
active_users$churn[i] = "CHURN"
}
else{
active_users$churn[i] = "NOCHURN"
}
}
b = table(active_users$churn)
pct_churn = b[1]/(b[1] + b[2])
as.double(pct_churn)
frequenze_general = c("week_access", "week_mission", "week_prod", "week_reward", "week_prodpoints", "week_misspoints", "week_rewpoints", "churn")
frequencies_general_data = select(active_users, frequenze_general)
M<-cor(frequencies_general_data[-8])
corrplot(M, method="number")
# Load Dataset
dataset <- frequencies_general_data
dataset$churn = as.factor(dataset$churn)
x <- dataset[,1:7]
y <- dataset[,8]
folds <- 5
bal_acc_rf=c()
bal_acc_dt=c()
bal_acc_lr=c()
auc_rf=c()
auc_dt=c()
auc_lr=c()
precision_rf=c()
precision_dt=c()
precision_lr=c()
recall_rf=c()
recall_dt=c()
recall_lr=c()
#Definizione dei dataset dell'outer loop
cvIndex <- createMultiFolds(y, folds, times=10)
for (i in 1:length(cvIndex)) {
#Definizione dei dataset da usare nell'inner loop
dataset_train_outer=dataset[cvIndex[[i]],]
dataset_test_outer=dataset[-cvIndex[[i]],]
x_train_outer=x[cvIndex[[i]],]
x_test_outer=x[-cvIndex[[i]],]
y_train_outer=y[cvIndex[[i]]]
y_test_outer=y[-cvIndex[[i]]]
recipe <- recipe(churn~., data=dataset_train_outer)
folds <- vfold_cv(dataset_train_outer, strata = churn, v = 5)
set.seed(123)
####################### Logistic Regression  ########################
lg <- logistic_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
lg_grid <- grid_regular(mixture(),
penalty(),levels=1)
lg_wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(lg)
lg_res <-
lg_wf %>%
tune_grid(
resamples = folds,
grid = lg_grid,
metrics = metric_set(yardstick::recall)
)
best_lg <- lg_res %>%
select_best("recall")
lr <- logistic_reg(mixture = best_lg$mixture, penalty = best_lg$penalty)
lr_fit <-
lr %>%
set_engine("glmnet") %>%
fit(churn ~., data = dataset_train_outer)
predictions_lr=predict(lr_fit, dataset_test_outer)
predictions_numeric = as.character(predictions_lr$.pred_class)
predictions_numeric[predictions_numeric =='NOCHURN'] = 0
predictions_numeric[predictions_numeric =='CHURN'] = 1
predictions_numeric = as.numeric(predictions_numeric)
bal_acc_lr[i]=bal_accuracy_vec(y_test_outer,predictions_lr$.pred_class)
auc_lr[i]=yardstick::roc_auc_vec(y_test_outer,predictions_numeric)
precision_lr[i]=yardstick::precision_vec(y_test_outer,predictions_lr$.pred_class)
recall_lr[i]=yardstick::recall_vec(y_test_outer,predictions_lr$.pred_class)
####################### Decision Tree  ########################
dt_spec <-
decision_tree(
cost_complexity = tune(),
tree_depth = tune()
) %>%
set_engine("rpart") %>%
set_mode("classification")
tree_grid <- grid_regular(cost_complexity(),
tree_depth(),
levels = 1)
tree_wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(dt_spec)
tree_res <-
tree_wf %>%
tune_grid(
resamples = folds,
grid = tree_grid,
metrics = metric_set(yardstick::recall)
)
best_tree <- tree_res %>%
select_best("recall")
dt <- decision_tree(cost_complexity = best_tree$cost_complexity, tree_depth=best_tree$tree_depth, mode = 'classification')
dt_fit <-
dt %>%
set_engine("rpart") %>%
fit(churn ~., data = dataset_train_outer)
predictions_dt=predict(dt_fit, dataset_test_outer)
predictions_numeric = as.character(predictions_dt$.pred_class)
predictions_numeric[predictions_numeric =='NOCHURN'] = 0
predictions_numeric[predictions_numeric =='CHURN'] = 1
predictions_numeric = as.numeric(predictions_numeric)
bal_acc_dt[i]=bal_accuracy_vec(y_test_outer,predictions_dt$.pred_class)
auc_dt[i]=yardstick::roc_auc_vec(y_test_outer,predictions_numeric)
precision_dt[i]=yardstick::precision_vec(y_test_outer,predictions_dt$.pred_class)
recall_dt[i]=yardstick::recall_vec(y_test_outer,predictions_dt$.pred_class)
####################### Random Forest  ########################
#Inner loop
rf_spec <- rand_forest(
mtry = tune(), trees = tune(), min_n = tune()
) %>%
set_engine(
"ranger", num.threads = 7, importance = "impurity"
) %>%
set_mode("classification")
rf_Wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(rf_spec)
rf_grid <-
grid_regular(
min_n(),
mtry(range = c(2,7)),
trees(range = c(10, 300)),levels = 1)
tune_res <-
rf_Wf %>%
tune_grid(
resamples = folds, grid = rf_grid,
metrics = metric_set(yardstick::recall)
)
best_rf <- tune_res %>%
select_best("recall")
rf <- rand_forest(mtry=best_rf$mtry, trees = best_rf$trees, min_n = best_rf$min_n, mode = 'classification')
rf_fit <-
rf %>%
set_engine("ranger") %>%
fit(churn ~., data = dataset_train_outer)
predictions_rf=predict(rf_fit, dataset_test_outer)
autoplot(tune_res)
predictions_numeric = as.character(predictions_rf$.pred_class)
predictions_numeric[predictions_numeric =='NOCHURN'] = 0
predictions_numeric[predictions_numeric =='CHURN'] = 1
predictions_numeric = as.numeric(predictions_numeric)
#valore di metrica dell'outer loop
bal_acc_rf[i]=bal_accuracy_vec(y_test_outer,predictions_rf$.pred_class)
auc_rf[i]=yardstick::roc_auc_vec(y_test_outer,predictions_numeric)
precision_rf[i]=yardstick::precision_vec(y_test_outer,predictions_rf$.pred_class)
recall_rf[i]=yardstick::recall_vec(y_test_outer,predictions_rf$.pred_class)
}
for (i in 1:length(cvIndex)) {
print(i)
#Definizione dei dataset da usare nell'inner loop
dataset_train_outer=dataset[cvIndex[[i]],]
dataset_test_outer=dataset[-cvIndex[[i]],]
x_train_outer=x[cvIndex[[i]],]
x_test_outer=x[-cvIndex[[i]],]
y_train_outer=y[cvIndex[[i]]]
y_test_outer=y[-cvIndex[[i]]]
recipe <- recipe(churn~., data=dataset_train_outer)
folds <- vfold_cv(dataset_train_outer, strata = churn, v = 5)
set.seed(123)
####################### Logistic Regression  ########################
lg <- logistic_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
lg_grid <- grid_regular(mixture(),
penalty(),levels=1)
lg_wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(lg)
lg_res <-
lg_wf %>%
tune_grid(
resamples = folds,
grid = lg_grid,
metrics = metric_set(yardstick::recall)
)
best_lg <- lg_res %>%
select_best("recall")
lr <- logistic_reg(mixture = best_lg$mixture, penalty = best_lg$penalty)
lr_fit <-
lr %>%
set_engine("glmnet") %>%
fit(churn ~., data = dataset_train_outer)
predictions_lr=predict(lr_fit, dataset_test_outer)
predictions_numeric = as.character(predictions_lr$.pred_class)
predictions_numeric[predictions_numeric =='NOCHURN'] = 0
predictions_numeric[predictions_numeric =='CHURN'] = 1
predictions_numeric = as.numeric(predictions_numeric)
bal_acc_lr[i]=bal_accuracy_vec(y_test_outer,predictions_lr$.pred_class)
auc_lr[i]=yardstick::roc_auc_vec(y_test_outer,predictions_numeric)
precision_lr[i]=yardstick::precision_vec(y_test_outer,predictions_lr$.pred_class)
recall_lr[i]=yardstick::recall_vec(y_test_outer,predictions_lr$.pred_class)
####################### Decision Tree  ########################
dt_spec <-
decision_tree(
cost_complexity = tune(),
tree_depth = tune()
) %>%
set_engine("rpart") %>%
set_mode("classification")
tree_grid <- grid_regular(cost_complexity(),
tree_depth(),
levels = 1)
tree_wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(dt_spec)
tree_res <-
tree_wf %>%
tune_grid(
resamples = folds,
grid = tree_grid,
metrics = metric_set(yardstick::recall)
)
best_tree <- tree_res %>%
select_best("recall")
dt <- decision_tree(cost_complexity = best_tree$cost_complexity, tree_depth=best_tree$tree_depth, mode = 'classification')
dt_fit <-
dt %>%
set_engine("rpart") %>%
fit(churn ~., data = dataset_train_outer)
predictions_dt=predict(dt_fit, dataset_test_outer)
predictions_numeric = as.character(predictions_dt$.pred_class)
predictions_numeric[predictions_numeric =='NOCHURN'] = 0
predictions_numeric[predictions_numeric =='CHURN'] = 1
predictions_numeric = as.numeric(predictions_numeric)
bal_acc_dt[i]=bal_accuracy_vec(y_test_outer,predictions_dt$.pred_class)
auc_dt[i]=yardstick::roc_auc_vec(y_test_outer,predictions_numeric)
precision_dt[i]=yardstick::precision_vec(y_test_outer,predictions_dt$.pred_class)
recall_dt[i]=yardstick::recall_vec(y_test_outer,predictions_dt$.pred_class)
####################### Random Forest  ########################
#Inner loop
rf_spec <- rand_forest(
mtry = tune(), trees = tune(), min_n = tune()
) %>%
set_engine(
"ranger", num.threads = 7, importance = "impurity"
) %>%
set_mode("classification")
rf_Wf <- workflow() %>%
add_recipe(recipe) %>%
add_model(rf_spec)
rf_grid <-
grid_regular(
min_n(),
mtry(range = c(2,7)),
trees(range = c(10, 300)),levels = 1)
tune_res <-
rf_Wf %>%
tune_grid(
resamples = folds, grid = rf_grid,
metrics = metric_set(yardstick::recall)
)
best_rf <- tune_res %>%
select_best("recall")
rf <- rand_forest(mtry=best_rf$mtry, trees = best_rf$trees, min_n = best_rf$min_n, mode = 'classification')
rf_fit <-
rf %>%
set_engine("ranger") %>%
fit(churn ~., data = dataset_train_outer)
predictions_rf=predict(rf_fit, dataset_test_outer)
autoplot(tune_res)
predictions_numeric = as.character(predictions_rf$.pred_class)
predictions_numeric[predictions_numeric =='NOCHURN'] = 0
predictions_numeric[predictions_numeric =='CHURN'] = 1
predictions_numeric = as.numeric(predictions_numeric)
#valore di metrica dell'outer loop
bal_acc_rf[i]=bal_accuracy_vec(y_test_outer,predictions_rf$.pred_class)
auc_rf[i]=yardstick::roc_auc_vec(y_test_outer,predictions_numeric)
precision_rf[i]=yardstick::precision_vec(y_test_outer,predictions_rf$.pred_class)
recall_rf[i]=yardstick::recall_vec(y_test_outer,predictions_rf$.pred_class)
}
